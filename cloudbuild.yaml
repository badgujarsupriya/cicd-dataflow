steps:
  # Build the Docker image for testing
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/dataflow-test:$SHORT_SHA', '.']
    
  # Run the tests in the Docker container
  - name: 'gcr.io/cloud-builders/docker'
    args: ['run', 'gcr.io/$PROJECT_ID/dataflow-test:$SHORT_SHA']
    
  # Upload the pipeline script to GCS
  - name: 'gcr.io/cloud-builders/gsutil'
    args: ['cp', 'src/cicd_pipeline.py', 'gs://$PROJECT_ID-dataflow-scripts/pipeline-$SHORT_SHA.py']
    
  # Create sample data and generate timestamp
  - name: 'gcr.io/cloud-builders/gsutil'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Creating sample data for testing..."
        echo "Hello World" > sample.txt
        echo "This is a test" >> sample.txt
        echo "For Dataflow pipeline" >> sample.txt
        
        # Get current timestamp
        timestamp=$(date +%Y%m%d%H%M%S)
        
        # Upload to GCS with timestamp
        gsutil cp sample.txt gs://$PROJECT_ID-data/input/sample-$timestamp.txt
        
        # Save the timestamp for the next step
        echo "$timestamp" > /workspace/timestamp.txt
        
  # Upload the DAG file directly to the Composer DAGs bucket
  - name: 'gcr.io/cloud-builders/gsutil'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        # Get timestamp
        timestamp=$(cat /workspace/timestamp.txt)
        
        # Create a temporary version of the DAG with all variables filled
        sed "s/PROJECT_ID = \"dataflow-cicd-453916\"/PROJECT_ID = \"$PROJECT_ID\"/g" dags/cicd_dataflow_dag.py > /workspace/filled_dag.py
        
        # Upload the DAG file to Composer's DAG bucket
        gsutil cp /workspace/filled_dag.py gs://us-central1-my-composer-env-a7ed6859-bucket/dags/cicd_dataflow_dag.py
        
        echo "DAG file uploaded to Composer bucket"
        
        # Wait for the DAG to be processed by Airflow
        echo "Waiting 60 seconds for DAG to be processed..."
        sleep 60

  # Trigger the DAG in Cloud Composer
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        # Get the timestamp from previous step
        timestamp=$(cat /workspace/timestamp.txt)
        
        echo "Triggering Airflow DAG to run Dataflow job..."
        gcloud composer environments run \
          $_COMPOSER_ENV \
          --location=$_COMPOSER_REGION \
          dags trigger -- cicd_dataflow_dag \
          -c '{"pipeline_file":"gs://$PROJECT_ID-dataflow-scripts/pipeline-$SHORT_SHA.py","input_file":"gs://$PROJECT_ID-data/input/sample-'"$timestamp"'.txt","output_path":"gs://$PROJECT_ID-data/output/results-'"$timestamp"'","project_id":"$PROJECT_ID"}'

# Save the Docker image to Container Registry
images:
  - 'gcr.io/$PROJECT_ID/dataflow-test:$SHORT_SHA'

# Add logging options to avoid service account errors
options:
  logging: CLOUD_LOGGING_ONLY

# Timeout for the build (30 minutes)
timeout: '1800s'

# Substitutions for variables
substitutions:
  _COMPOSER_ENV: 'my-composer-environment'  # Default value for your Composer environment name
  _COMPOSER_REGION: 'us-central1'           # Default value for your Composer region
